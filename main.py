"""
NICL í”„ë¡œì íŠ¸ - ë©”ì¸ ì‹¤í–‰ íŒŒì¼
News Information Collection & Library
"""

import os
import sys
import argparse
from datetime import datetime

# ë””ë²„ê·¸ìš© ì¶œë ¥ (ë¬¸ì œ í•´ê²° ì‹œ ì œê±° ê°€ëŠ¥)
print("=" * 60)
print("NICL í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ì¤‘...")
print("í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:", os.getcwd())

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.abspath(__file__))
print("í”„ë¡œì íŠ¸ ë£¨íŠ¸:", project_root)

if project_root not in sys.path:
    sys.path.insert(0, project_root)
    print("âœ… Python ê²½ë¡œì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ë¨")

try:
    from src.news_collector import NewsCollector
    print("âœ… NewsCollector ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ NewsCollector ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("\në¬¸ì œ í•´ê²° ë°©ë²•:")
    print("1. ëª¨ë“  íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸")
    print("2. src í´ë” ë‚´ __init__.py íŒŒì¼ë“¤ í™•ì¸")
    print("3. pip install -r requirements.txt ì‹¤í–‰")
    sys.exit(1)

print("=" * 60)
print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='NICL ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ (API + í¬ë¡¤ë§)')

    # ë©”ì¸ ê¸°ëŠ¥: ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
    parser.add_argument('--latest', '-l', action='store_true', help='ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ (ë©”ì¸ ê¸°ëŠ¥)')
    parser.add_argument('--count', '-c', type=int, default=50, help='ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸: 50)')

    # ë¶€ê°€ ê¸°ëŠ¥: í‚¤ì›Œë“œ ê¸°ë°˜ ìˆ˜ì§‘
    parser.add_argument('--keyword', '-k', type=str, help='í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰')
    parser.add_argument('--keywords', '-ks', nargs='+', help='ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)')
    parser.add_argument('--category', type=str, help='ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬')
    parser.add_argument('--section', type=str, choices=['politics', 'economy', 'society', 'culture', 'world', 'it'],
                       help='ë‰´ìŠ¤ ì„¹ì…˜ë³„ ìˆ˜ì§‘')
    parser.add_argument('--trending', '-t', action='store_true', help='ì¸ê¸° ë‰´ìŠ¤ ìˆ˜ì§‘')

    # ìœ í‹¸ë¦¬í‹°
    parser.add_argument('--stats', '-s', action='store_true', help='ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ í™•ì¸')
    parser.add_argument('--validate', '-v', action='store_true', help='ì„¤ì • ê²€ì¦')

    # ìˆ˜ì§‘ ë°©ì‹ ì„ íƒ
    parser.add_argument('--api-only', action='store_true', help='APIë§Œ ì‚¬ìš©')
    parser.add_argument('--crawl-only', action='store_true', help='í¬ë¡¤ë§ë§Œ ì‚¬ìš©')
    # ê¸°ë³¸ê°’: API + í¬ë¡¤ë§ ë‘˜ ë‹¤ ì‚¬ìš©

    args = parser.parse_args()
    
    # ìˆ˜ì§‘ ë°©ì‹ ê²°ì •
    use_api = not args.crawl_only
    use_crawling = not args.api_only
    
    # ì¸ìˆ˜ê°€ ì—†ìœ¼ë©´ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ (ê¸°ë³¸ ë™ì‘)
    if len(sys.argv) == 1:
        args.latest = True
        print("ì¸ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        print("ìì„¸í•œ ì‚¬ìš©ë²•ì€ 'python main.py --help'ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
        print()

    # ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    try:
        with NewsCollector() as collector:
            print("=" * 60)
            print("NICL (News Information Collection & Library)")
            print("ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ (API + ì›¹ í¬ë¡¤ë§)")
            print("=" * 60)
            print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ìˆ˜ì§‘ ë°©ì‹: {'API' if use_api else ''}{' + ' if (use_api and use_crawling) else ''}{'í¬ë¡¤ë§' if use_crawling else ''}")
            print()

            # ì„¤ì • ê²€ì¦
            if args.validate:
                print("ğŸ” ì„¤ì • ê²€ì¦ ì¤‘...")
                if collector.validate_setup():
                    print("âœ… ëª¨ë“  ì„¤ì •ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print("âŒ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return
            
            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
            if args.stats:
                print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„")
                print("-" * 30)
                stats = collector.get_database_statistics()

                print(f"ì´ ë‰´ìŠ¤ ê¸°ì‚¬: {stats.get('total_articles', 0):,}ê°œ")
                print(f"ê³ ìœ  ë‰´ìŠ¤: {stats.get('unique_articles', 0):,}ê°œ")
                print(f"ì¤‘ë³µ ë‰´ìŠ¤: {stats.get('total_duplicates', 0):,}ê°œ")
                print()
                return

            # ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ (ë©”ì¸ ê¸°ëŠ¥)
            if args.latest:
                print("ğŸ“° ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                print(f"ìˆ˜ì§‘ ëª©í‘œ: {args.count}ê°œ")
                print("-" * 40)

                result = collector.collect_latest_news(
                    max_count=args.count,
                    use_api=use_api,
                    use_crawling=use_crawling
                )

                if result['success']:
                    print("âœ… ìˆ˜ì§‘ ì™„ë£Œ!")
                    print(f"ğŸ“° ì´ ìˆ˜ì§‘: {result['collected']}ê°œ")
                    print(f"   â”œâ”€ API: {result.get('api_count', 0)}ê°œ")
                    print(f"   â””â”€ í¬ë¡¤ë§: {result.get('crawl_count', 0)}ê°œ")
                    print(f"ğŸ’¾ ì €ì¥ë¨: {result['saved']}ê°œ")
                    print(f"ğŸ”„ ì¤‘ë³µ: {result['duplicates']}ê°œ")
                    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
                else:
                    print("âŒ ìˆ˜ì§‘ ì‹¤íŒ¨!")
                    if 'error' in result:
                        print(f"ì˜¤ë¥˜: {result['error']}")

                return

            # ì„¹ì…˜ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
            if args.section:
                print(f"ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ '{args.section}' ì„¹ì…˜ ìˆ˜ì§‘ ì¤‘...")
                print(f"ìˆ˜ì§‘ ëª©í‘œ: {args.count}ê°œ")
                print("-" * 40)
                
                result = collector.collect_news_by_section(
                    section=args.section,
                    max_count=args.count
                )
                
                if result['success']:
                    print("âœ… ìˆ˜ì§‘ ì™„ë£Œ!")
                    print(f"ğŸ“° ì´ ìˆ˜ì§‘: {result['collected']}ê°œ")
                    print(f"ğŸ’¾ ì €ì¥ë¨: {result['saved']}ê°œ")
                    print(f"ğŸ”„ ì¤‘ë³µ: {result['duplicates']}ê°œ")
                    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
                else:
                    print("âŒ ìˆ˜ì§‘ ì‹¤íŒ¨!")
                    if 'error' in result:
                        print(f"ì˜¤ë¥˜: {result['error']}")
                
                return
            
            # ì¸ê¸° ë‰´ìŠ¤ ìˆ˜ì§‘
            if args.trending:
                print("ğŸ”¥ ì¸ê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                news_list = collector.get_trending_news(limit=args.count)
                
                if news_list:
                    print(f"âœ… {len(news_list)}ê°œì˜ ì¸ê¸° ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    for i, news in enumerate(news_list[:5], 1):
                        print(f"{i}. {news.title[:50]}...")
                else:
                    print("âŒ ì¸ê¸° ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return
            
            # ë‹¨ì¼ í‚¤ì›Œë“œ ìˆ˜ì§‘
            if args.keyword:
                print(f"ğŸ” í‚¤ì›Œë“œ '{args.keyword}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                print(f"ìˆ˜ì§‘ ëª©í‘œ: {args.count}ê°œ")
                if args.category:
                    print(f"ì¹´í…Œê³ ë¦¬: {args.category}")
                print("-" * 40)
                
                result = collector.collect_news_by_keyword(
                    keyword=args.keyword,
                    max_count=args.count,
                    category=args.category,
                    use_api=use_api,
                    use_crawling=use_crawling
                )
                
                if result['success']:
                    print("âœ… ìˆ˜ì§‘ ì™„ë£Œ!")
                    print(f"ğŸ“° ì´ ìˆ˜ì§‘: {result['collected']}ê°œ")
                    print(f"   â”œâ”€ API: {result.get('api_count', 0)}ê°œ")
                    print(f"   â””â”€ í¬ë¡¤ë§: {result.get('crawl_count', 0)}ê°œ")
                    print(f"ğŸ’¾ ì €ì¥ë¨: {result['saved']}ê°œ")
                    print(f"ğŸ”„ ì¤‘ë³µ: {result['duplicates']}ê°œ")
                    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
                else:
                    print("âŒ ìˆ˜ì§‘ ì‹¤íŒ¨!")
                    if 'error' in result:
                        print(f"ì˜¤ë¥˜: {result['error']}")
                
                return
            
            # ë‹¤ì¤‘ í‚¤ì›Œë“œ ìˆ˜ì§‘
            if args.keywords:
                print(f"ğŸ” ë‹¤ì¤‘ í‚¤ì›Œë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                print(f"í‚¤ì›Œë“œ: {', '.join(args.keywords)}")
                print(f"í‚¤ì›Œë“œë‹¹ ìˆ˜ì§‘ ëª©í‘œ: {args.count}ê°œ")
                print("-" * 40)
                
                results = collector.collect_news_by_keywords(
                    keywords=args.keywords,
                    max_count_per_keyword=args.count,
                    use_api=use_api,
                    use_crawling=use_crawling
                )
                
                # ê²°ê³¼ ìš”ì•½
                total_collected = sum(r['collected'] for r in results)
                total_saved = sum(r['saved'] for r in results)
                total_duplicates = sum(r['duplicates'] for r in results)
                total_api = sum(r.get('api_count', 0) for r in results)
                total_crawl = sum(r.get('crawl_count', 0) for r in results)
                success_count = sum(1 for r in results if r['success'])
                
                print("\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
                print("-" * 30)
                print(f"ì²˜ë¦¬ëœ í‚¤ì›Œë“œ: {len(args.keywords)}ê°œ")
                print(f"ì„±ê³µí•œ í‚¤ì›Œë“œ: {success_count}ê°œ")
                print(f"ì´ ìˆ˜ì§‘: {total_collected}ê°œ")
                print(f"   â”œâ”€ API: {total_api}ê°œ")
                print(f"   â””â”€ í¬ë¡¤ë§: {total_crawl}ê°œ")
                print(f"ì´ ì €ì¥: {total_saved}ê°œ")
                print(f"ì´ ì¤‘ë³µ: {total_duplicates}ê°œ")
                
                print("\nğŸ“ í‚¤ì›Œë“œë³„ ìƒì„¸ ê²°ê³¼")
                print("-" * 40)
                for result in results:
                    status = "âœ…" if result['success'] else "âŒ"
                    print(f"{status} {result['keyword']}: "
                          f"ìˆ˜ì§‘={result['collected']} (API:{result.get('api_count',0)}, í¬ë¡¤:{result.get('crawl_count',0)}), "
                          f"ì €ì¥={result['saved']}, ì¤‘ë³µ={result['duplicates']}")
                
                return
            
            # ê¸°ë³¸ ë„ì›€ë§
            print("ì‚¬ìš© ì˜ˆì‹œ:")
            print("\n[ë©”ì¸ ê¸°ëŠ¥ - ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘]")
            print("python main.py                    # ìµœì‹  ë‰´ìŠ¤ 50ê°œ ìˆ˜ì§‘ (ê¸°ë³¸)")
            print("python main.py --latest           # ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘")
            print("python main.py -l --count 100     # ìµœì‹  ë‰´ìŠ¤ 100ê°œ ìˆ˜ì§‘")
            print("python main.py -l --api-only      # APIë§Œ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘")
            print("python main.py -l --crawl-only    # í¬ë¡¤ë§ë§Œ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘")

            print("\n[ë¶€ê°€ ê¸°ëŠ¥ - í‚¤ì›Œë“œ ê¸°ë°˜ ìˆ˜ì§‘]")
            print("python main.py --keyword 'ì¸ê³µì§€ëŠ¥' --count 20")
            print("python main.py -k 'ì±—GPT' -c 30 --api-only")
            print("python main.py --keywords 'ì •ì¹˜' 'ê²½ì œ' 'ì‚¬íšŒ' --count 10")
            print("python main.py --section politics --count 30")
            print("python main.py --trending --count 30")

            print("\n[ìœ í‹¸ë¦¬í‹°]")
            print("python main.py --stats            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„")
            print("python main.py --validate         # ì„¤ì • ê²€ì¦")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()